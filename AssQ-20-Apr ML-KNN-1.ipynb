{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c11665-1ae2-4531-bb8b-e788b9145630",
   "metadata": {},
   "source": [
    "# AssQ 20-Apr ML-KNN-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e89112e-36aa-4bc8-8728-15c5665ab47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the KNN algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14b845c-7ca8-48c7-8f47-91e389675e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, \n",
    "which uses proximity to make classifications or predictions about the grouping of an individual data point.\n",
    "\n",
    "K-nearest neighbors (KNN) algorithm is a type of supervised ML algorithm which can be used \n",
    "for both classification as well as regression predictive problems.\n",
    "However, it is mainly used for classification predictive problems in industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db8fa63-bbbe-48a4-9f53-c740074a359f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df856f8-ff93-4bf8-8dbc-3e46e2188dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. How do you choose the value of K in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d38a6-1762-473e-bae3-d62a1c498b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of k will largely depend on the input data as data with more outliers or \n",
    "noise will likely perform better with higher values of k. Overall, it is recommended \n",
    "to have an odd number for k to avoid ties in classification,\n",
    "and cross-validation tactics can help you choose the optimal k for your dataset.\n",
    "\n",
    "Note that if k is chosen as the total number of observations in the Training Set,\n",
    "all the observations in the Training Set become nearest neighbors.\n",
    "The default value for this option is 1.\n",
    "\n",
    "K should be the square root of n (number of data points in the training dataset).\n",
    "K should be chosen as the odd so that there are no ties.\n",
    "If the square root is even, then add or subtract 1 to it.\n",
    "\n",
    "Each instance in essence votes for their class and the class with the most votes is taken as the prediction.\n",
    "If you are using K and you have an even number of classes (e.g. 2)\n",
    "it is a good idea to choose a K value with an odd number to avoid a tie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8671c6a6-57f7-4d32-bc2d-5e175b09166c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326d036-0056-4642-9180-da29ee2a3eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe2182-ad51-44c8-974b-c89c0ea8ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The difference between the KNN classifier and KNN regression methods is that the classifier\n",
    "is used in situations where the response variable is categorical (qualitative),\n",
    "while the regressor is used in numerical situations (quantitative).\n",
    "\n",
    "The key differences are:\n",
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627dd861-4b99-4df6-b616-b3c64913b742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae6f25-ba7a-43e5-8e84-6202074964d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. How do you measure the performance of KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748885e9-db24-418d-a3fd-a5fcbdc181f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main concept for k-NN depends on calculating the distances between the tested, \n",
    "and the training data samples in order to identify its nearest neighbours.\n",
    "The tested sample is then simply assigned to the class of its nearest neighbour\n",
    "\n",
    "Evaluation procedure 1 - Train and test on the entire dataset. Train the model on the entire dataset. \n",
    "Test the model on the same dataset, \n",
    "and evaluate how well we did by comparing the predicted response values with the true response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45506ee3-1d40-4092-a7e4-f0ac0a2148e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f39187-1409-4201-b72f-c97bef816833",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. What is the curse of dimensionality in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ff1c8-6eae-424a-9c4b-247d81db0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "The “Curse of Dimensionality” is a tongue in cheek way of stating that there's a ton of space in high-dimensional data sets.\n",
    "The size of the data space grows exponentially with the number of dimensions.\n",
    "This means that the size of your data set must also grow exponentially in order to keep the same density.\n",
    "\n",
    "Definition. The curse of dimensionality, first introduced by Bellman [1], indicates that the number of \n",
    "samples needed to estimate an arbitrary function with a given level of\n",
    "accuracy grows exponentially with respect to the number of input variables (i.e., dimensionality) of the function.\n",
    "\n",
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. \n",
    "It refers to the fact that algorithms are harder\n",
    "to design in high dimensions and often have a running time exponential in the dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84bfce8-f95e-4015-9c6f-b02ef6262f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ac69b1-22dc-4fa0-bbe7-a36894889d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q6. How do you handle missing values in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877bc6f-ff1f-4141-adb5-bba2a80c9b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "The idea in kNN methods is to identify 'k' samples in the dataset that are similar or close in the space.\n",
    "Then we use these 'k' samples to estimate the value of the missing data points.\n",
    "Each sample's missing values are imputed using the mean value of the 'k'-neighbors found in the dataset.\n",
    "\n",
    "The k-NN algorithm can be used for imputing the missing value of both categorical and continuous variables. That is true.\n",
    "k-NN can be used as one of many techniques when it comes to handling missing values.\n",
    "\n",
    "A shared sense of identity (Essence of kNN algorithm)\n",
    "\n",
    "In such scenarios, algorithms like k-Nearest Neighbors (kNN) can help to impute the values of missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b032833-b782-4241-8b49-24ceca41ca05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5327ef-ccfb-4463-abf3-8523c8a6e97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e1f62c-64ba-472b-87c6-1b54f8278876",
   "metadata": {},
   "outputs": [],
   "source": [
    "The key differences are: KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability.\n",
    "\n",
    "\n",
    "KNN algorithm can be used for both classification and regression problems. \n",
    "The KNN algorithm uses 'feature similarity' to predict the values of any new data points.\n",
    "This means that the new point is assigned a value based on how closely it resembles the points in the training set.\n",
    "\n",
    "KNN is one of the simplest forms of machine learning algorithms mostly used for classification.\n",
    "It classifies the data point on how its neighbor is classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6721b-7f8c-4b1a-90e3-bd0234a4e02b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd601c5-59fa-41db-8fdf-fad8cb563c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c3038-c25b-42fb-afb3-4b076bb21036",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages and disadvantages of KNN\n",
    "It's easy to understand and simple to implement.\n",
    "It can be used for both classification and regression problems.\n",
    "It's ideal for non-linear data since there's no assumption about underlying data.\n",
    "It can naturally handle multi-class cases.\n",
    "It can perform well with enough representative data.\n",
    "\n",
    "\n",
    "It has advantages - nonparametric architecture, simple and powerful, requires no traning time, \n",
    "but it also has disadvantage - memory intensive, classification and estimation are slow.\n",
    "\n",
    "It's main disadvantages are that it is quite computationally inefficient and \n",
    "its difficult to pick the “correct” value of K. However, the advantages of\n",
    "this algorithm is that it is versatile to different calculations of proximity, \n",
    "it's very intuitive and that it's a memory based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef089d97-d853-4962-b79a-cd7ffd2c782f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da387400-e410-40f4-93d1-84234ae1bbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c95a48-dfb1-42fd-840a-06ab01786b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean distance is the shortest path between source and destination which is a straight line .\n",
    "but Manhattan distance is sum of all the real distances between source(s) and destination(d)\n",
    "and each distance are always the straight lines.\n",
    "\n",
    "Manhattan distance is usually preferred over the more common Euclidean distance when there is\n",
    "high dimensionality in the data. Hamming distance is used to measure the distance between categorical variables, \n",
    "and the Cosine distance metric is mainly used to find the amount of similarity between two data points.\n",
    "\n",
    "\n",
    "We don't use Manhattan Distance, because it calculates distance horizontally or vertically only.\n",
    "It has dimension restrictions. On the other hand, the Euclidean metric can be used in any space to calculate distance.\n",
    "Since the data points can be represented in any dimension, it is a more viable option.\n",
    "\n",
    "From the above equation you notice that the formula is the same as Euclidean distance but the change is \n",
    "that here we prefer the value of P, So if we take the P-value equals to 2 then it is euclidian distance \n",
    "and takes P-value equals to 1 then it is considered as Manhatten distance.\n",
    "\n",
    "The Manhattan distance, also called the Taxicab distance or the City Block distance,\n",
    "calculates the distance between two real-valued vectors. \n",
    "It is perhaps more useful to vectors that describe objects on a uniform grid, like a chessboard or city blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540840e-6479-4021-8daa-6939d1947882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12d510-195e-489f-8c0d-9c29c69aca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06bc7b-b07b-4f47-a76d-bc5ab9d73d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature scaling is essential for machine learning algorithms that calculate distances between data. \n",
    "If not scaled, the feature with a higher value range starts dominating when calculating distances. \n",
    "KNN which uses Euclidean distance is one such algorithm which essentially require scaling.\n",
    "\n",
    "Feature scaling (standardization and normalization) is required before applying the KNN algorithm to any dataset.\n",
    "\n",
    "Yes, feature scaling is required to get the better performance of the KNN algorithm.\n",
    "For Example, Imagine a dataset having n number of instances and N number of features.\n",
    "There is one feature having values ranging between 0 and 1.\n",
    "Meanwhile, there is also a feature that varies from -999 to 999.\n",
    "\n",
    "Feature scaling is the process of normalising the range of features in a dataset.\n",
    "Real-world datasets often contain features that are varying in degrees of magnitude, range and units.\n",
    "Therefore, in order for machine learning models to interpret these features on the same scale, we need to perform feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0c3287-c79a-4cc7-98fe-8c3ea58d712a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09fbee-60fb-4e61-9069-0fe53e0f0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "...............................................The End.............................................."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
